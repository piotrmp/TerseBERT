# TerseBERT

This repository contains information and code for *TerseBERT*, a pretrained language model created by fine-tuning [BERT](https://github.com/google-research/bert). TerseBERT is not only able to predict, which word is most likely in a given context (like a regular language model), but if any word is necessary at all. It was created as a component of a text simplification solution described in the article *[Multi-Word Lexical Simplification](https://www.aclweb.org/anthology/TODO.pdf)* presented at the [COLING 2020](https://coling2020.org/) conference in Barcelona.

For example, consider the sentence *The fat cat sat on the mat.* 

If you need any more information consult [the paper](https://www.aclweb.org/anthology/TODO.pdf) or contact its authors! 

## Obtaining and using TerseBERT

## Training your own TerseBERT 
